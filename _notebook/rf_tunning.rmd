---
output:
    html_document:
        toc: yes
params:
    title: 'Random Forest Model - Tunning'
    working_folder: '/Users/xinliu/Desktop/rna_lib_analysis/2018_12_10-ML_Random_Forest-Tuning'
    
    # Dataset "codenames" - Use "comma" to sparate
    # dataset_codename_str: 'Hela_no_gRNA'
    dataset_codename_str: 'neil1_computational'

    # Testing related
    # Validation dataset "codenames" - Use "comma" to sparate
    external_test_dataset_codename_str: 'neil1_computational, ajuba_bc_computational, ttyh2_computational, ttyh2_ecs_bc_experimental, neil1_experimental, neil1_degenerate_computational, ttyh2_bc_degenerate_computational'
    # external_test_dataset_codename_str: 'neil1_computational, ajuba_bc_computational, ttyh2_bc_computational，ttyh2_ecs_bc_computational, ttyh2_ecs_bc_experimental, neil1_experimental, HEK293.WT1, Hela_no_gRNA, Hela_no_gRNA+IFN, u87_ctrl_vs_kd'
    # all datasets: 'neil1_computational, ajuba_bc_computational, ttyh2_computational, neil1_degenerate_computational, ttyh2_bc_degenerate_computational, ttyh2_bc_computational，ttyh2_ecs_bc_computational, ttyh2_ecs_bc_experimental, neil1_experimental, HEK293.WT1, Hela_no_gRNA, Hela_no_gRNA+IFN, u87_ctrl_vs_kd'

    # Tunning related
    # Training-validation dataset split
    training_validation_split: 0.8
    # Use "comma" to sparate
    feature_column_str: 'site_prev_nt, site_next_nt, num_mutations, mut_pos, mut_site_dist, mut_ref_nt, mut_nt, site_struct, site_length, site_length_interior_es, site_length_interior_ecs, site_3prm_cp_interior, d_all_stem_length, d2_struct, d1_distance, d1_length_stem, d2_distance, d3_distance, d3_length_stem, d2_length_interior_ecs, d2_length_interior_es, d2_3prm_cp_interior, d_count, u1_length_stem, u3_distance, u_all_stem_length, u2_distance, u1_distance, u2_length_stem, u2_struct, u_count'
    
    # RF Model File, relative to "working folder"
    # './models/rf_model-feature_picked-v5.rds'
    model_file: ''
# title: "Prediction of Adar Editing Levels - `r params$data_name` (`r params$data_type`)"
title: "Prediction of Adar Editing Levels - `r params$title`"
---

```{r "knitr config", cache = FALSE, include=FALSE}
require("knitr")
# 
script_folder_path = getwd()
if (params$working_folder != '') {
    # Set it "globally"
    opts_knit$set(root.dir = params$working_folder)
}

# Global Chunk Options
# Ref - https://yihui.name/knitr/options/
opts_chunk$set(
    echo = TRUE
)
```

```{r Info, include=FALSE}
# ## Purpose of This Script
# It aims to run "Random Forest" on the RNA Liv data. 
# 
# ## Tutorial of Rmarkdown
# - https://bookdown.org/yihui/rmarkdown/
```

```{r Libraries, include=FALSE}
#
library(knitr)

# Random Forest
library(mlbench)
library(caret)
library(randomForest)
library(gbm)

library(ggplot2)

library(doMC)  # Multi-core

# Tools
library(rlist)
library(gtools)
library(gsubfn)
library(GetoptLong) # String interpolation - https://cran.r-project.org/web/packages/GetoptLong/vignettes/variable_interpolation.html
```

```{r Setup, include=FALSE}
# Clear - NO NEED. It needs "passing params"
# rm(list=ls())

# Global options
options(warn=-1)

# Use multi-core
registerDoMC(cores=8)

# Local scripts
source(normalizePath(file.path(script_folder_path, './utils.R')))
source(normalizePath(file.path(script_folder_path, './helper.R')))
```


```{r Prep, include=FALSE}
# Set the "Seed" for following "Random Forest" running
seed_num = 166
set.seed(seed_num)

#
feature_columns = c('num_mutations', 'mut_site_dist', 'mut_pos', 'mut_nt', 'mut_ref_nt', 'mut_prev_struct', 'mut_same_as_site',
                    'site_length_interior_ecs', 'site_struct', 'site_length_interior_es', 'site_3prm_cp_interior',
                    'u_all_stem_length', 'u_count',
                    'd_all_stem_length', 'd_count', 
                    'u1_length_stem', 'u1_distance', 
                    'u2_distance', 'u2_length_stem', 'u2_struct', 
                    'u3_distance', 
                    'd1_distance', 'd1_length_stem', 
                    'd2_struct', 'd2_distance', 'd2_length_interior_ecs', 'd2_length_interior_es', 'd2_3prm_cp_interior', 
                    'd3_distance', 'd3_length_stem')
if (params$feature_column_str != '') {
    feature_columns = str_to_list(params$feature_column_str)
}

# RF Parameters
rf_mtry = sqrt(length(feature_columns))
rf_mtree = 500
# Metric for "model tuning"
rf_metric <- "RMSE"

# Check if the "MODEL" exists for loading
# Temp fix
# - Use `setwd()` tp set the "working folder" since `opts_knit$set(root.dir = params$working_folder)` not working.
# - Ignore "warning" in this chunk
if (params$working_folder != '') {
    setwd(params$working_folder)
}
model_exist = FALSE
rf_model = NULL
if (file.exists(params$model_file)) {
    model_exist = TRUE
    rf_model <- readRDS(params$model_file)
}

```

# Summary

This script helps "tune" the Random Forest model, by the following tunning parameters: 

- Focused "feature" list
- Training-validation split ratio of the "orginal" datasets
- Random Forest marameters

## Feature List

The following features are considered: 

```{r Feature list, echo=FALSE}

# 
for (feature_name in feature_columns) {
    #
    print(qq("- @{feature_name}"))
}

```


# Datasets Pre-processing

The following datasets are used as "training" & "validation": 

```{r  List datasets - training & validation, echo=FALSE }

# Build the list of "dataset codenames"
dataset_meta = c(
    'neil1_computational'
)
if (params$dataset_codename_str != '') {
    dataset_meta = str_to_list(params$dataset_codename_str)
}

# 
for (dataset_codename in dataset_meta) {
    #
    print(qq("- @{dataset_codename}"))
}

```

The following datasets are used for "testing": 

```{r  List datasets - testing, echo=FALSE }

# Build the list of "testing dataset codenames"
testing_dataset_meta = c()
if (params$external_test_dataset_codename_str != '') {
    testing_dataset_meta = str_to_list(params$external_test_dataset_codename_str)
}

# 
for (dataset_codename in testing_dataset_meta) {
    #
    print(qq("- @{dataset_codename}"))
}

```

## Original Datasets Summary

```{r Load "feature" data files, echo=FALSE, warning=FALSE}

# Temp fix
# - Use `setwd()` tp set the "working folder" since `opts_knit$set(root.dir = params$working_folder)` not working.
# - Ignore "warning" in this chunk
if (params$working_folder != '') {
    setwd(params$working_folder)
}

# Loop into "codenames" and build the "dataset variables"
for (dataset_codename in dataset_meta) {
    # file path
    file_path = qq("./_output/feature_matrices/@{dataset_codename}.features.csv")
    assign(qq("feature_file_relative_path.@{dataset_codename}"), file_path)
    
    print(file_path)
    
    # original data
    data_orig = read.csv(file=file_path, header=TRUE, sep=",")
    assign(qq("data_orig.@{dataset_codename}"), data_orig)
}

# Summary Orig Datasets
# for (dataset_codename in dataset_meta) {
#     #
#     print(qq("Original Dataset Summary - @{dataset_codename}"))
#     
#     dataset.name = qq("data_orig.@{dataset_codename}")
#     print(summary(get(dataset.name)))
#     # print(summary(eval(as.symbol(dataset_codename))))
# }

# Testing dataset
for (dataset_codename in testing_dataset_meta) {
    # file path
    file_path = qq("./_output/feature_matrices/@{dataset_codename}.features.csv")
    assign(qq("feature_file_relative_path.@{dataset_codename}"), file_path)
    
    print(file_path)
    
    # original data
    data_orig = read.csv(file=file_path, header=TRUE, sep=",")
    assign(qq("data_orig.@{dataset_codename}"), data_orig)
}
```

# Prediction with "Random Forest"

## Trainig-Validation Split

The "Training split" is ``r params$training_validation_split``. 

```{r Traing-Test Split, include=FALSE, warning=FALSE}

# Determine "Training" and "Test" set
# - Training set - a combined set of all "training" set from each data set
# - Test set - a combined set of all "training" set from each data set

data.combined = data.frame()
train_split.combined = data.frame()
validation_split.combined = data.frame()
for (dataset_codename in dataset_meta) {
    #
    dataset.name = qq("data_orig.@{dataset_codename}")
    dataset = get(dataset.name)
    
    train_indices.name = qq("train_indices.@{dataset_codename}")
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    
    assign(train_indices.name, 
           sample(nrow(dataset), as.double(params$training_validation_split) * nrow(dataset), replace=FALSE)
           )
    train_indices = get(train_indices.name)
    
    # Split
    assign(train_split.name, 
           dataset[ train_indices, ]
           )
    assign(validation_split.name, 
           dataset[ -train_indices, ]
           )
    
    data.combined = smartbind(data.combined, get(dataset.name))
    train_split.combined = smartbind(train_split.combined, get(train_split.name))
    validation_split.combined = smartbind(validation_split.combined, get(validation_split.name))
}

# For "Test" dataset
for (dataset_codename in testing_dataset_meta) {
    #
    dataset.name = qq("data_orig.@{dataset_codename}")
    dataset = get(dataset.name)
    
    # Drop "records" based on "editing level" value 
    dataset = dataset[ dataset[ , 'editing_value'] > 0.002, ]
    
    dataset_test.name = qq("data.test.@{dataset_codename}")
    assign(dataset_test.name, dataset)
}

# Keep the "levels" of overall "original" datasets
levels.data.combined = sapply(data.combined, levels)
```

## Processed Data Summary

```{r Process Dataset, include=FALSE, warning=FALSE}

# "Column Names" to be dropped in the ourput dataset. 
column_names_to_drop = c("rna_id")
# "Column Names" to be processed the "data value issue"
column_names_to_correct = c()

# Combined
data.combined = prepData(data.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_keep = feature_columns,
                    columns_to_correct = column_names_to_correct)
train_split.combined = prepData(train_split.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_keep = feature_columns, 
                    columns_to_correct = column_names_to_correct)
validation_split.combined = prepData(validation_split.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_keep = feature_columns, 
                    columns_to_correct = column_names_to_correct)

# For each dataset
for (dataset_codename in dataset_meta) {
    # 
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    assign(train_split.name,
           prepData(train_split, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_keep = feature_columns,
                columns_to_correct = column_names_to_correct)
           )
    assign(validation_split.name,
           prepData(validation_split, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_keep = feature_columns,
                columns_to_correct = column_names_to_correct)
           )
    
    # Convert "editing level" with "logarithm" calc
    # data$y = log2(data$y / (1 - data$y))
}

# For each "testing" dataset
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    assign(data_test.name,
           prepData(data_test, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_keep = feature_columns, 
                columns_to_correct = column_names_to_correct)
           )
}

# Fix the "Levals" of data frames to ensure "ALL" datasets are in the "SAME" levels. 
# 
# NOTE:
#   - The "fix" includes three steps: 
#       - Ensure the "processed combined" dataset has the "SAME" levels of "original combined" dataset. 
#       - If having the "testing" datasets, check them first, since they may bring some extra "levels". 
#       - Check if need to update the "subset" datasets of the "combined" dataset. 
# 

# For the "processed combined" dataset
for (col in names(data.combined)) {
    #
    if( length(levels(data.combined[ ,col ])) <= length(levels.data.combined[col]) ) {
        levels(data.combined[ ,col ]) = levels.data.combined[[col]]
    }
}

# Start to FIX
list[data.combined, train_split.combined] <- fixLevels(data.combined, train_split.combined)
list[data.combined, validation_split.combined] <- fixLevels(data.combined, validation_split.combined)

# For "testing" datasets
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    print(data_test.name)
    
    list[data, test] <- fixLevels(data.combined, data_test)
    data.combined <<- data
    assign(data_test.name, test)
}

# For subset datasets
for (dataset_codename in dataset_meta) {
    #
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    #
    list[data, train] <- fixLevels(data.combined, train_split)
    data.combined <<- data
    assign(train_split.name, train)
    
    # 
    list[data, validation] <- fixLevels(data.combined, validation_split)
    data.combined <<- data
    assign(validation_split.name, validation)
}

# Run is again
list[data.combined, train_split.combined] <- fixLevels(data.combined, train_split.combined)
list[data.combined, validation_split.combined] <- fixLevels(data.combined, validation_split.combined)

# For "testing" datasets
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    print(data_test.name)
    
    list[data, test] <- fixLevels(data.combined, data_test)
    data.combined <<- data
    assign(data_test.name, test)
}

# For subset datasets
for (dataset_codename in dataset_meta) {
    #
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    #
    list[data, train] <- fixLevels(data.combined, train_split)
    data.combined <<- data
    assign(train_split.name, train)
    
    # 
    list[data, validation] <- fixLevels(data.combined, validation_split)
    data.combined <<- data
    assign(validation_split.name, validation)
}
```

## Run Random Forest

```{r Rondom Forest Tuning - Determine "mtry", eval=FALSE, include=FALSE}

control <- trainControl(method="repeatedcv", number = 10, repeats = 3, 
                        # search="grid"
                        search="random"
                        )
#
set.seed(seed_num)
rf_search <- train(y~., data = train_split.combined, 
                   method = 'rf', 
                   metric = rf_metric, 
                   tuneLength = 15,
                   # tuneGrid = tunegrid, 
                   trControl = control)
summary(rf_search)
print(rf_search)
plot(rf_search)

```

```{r Rondom Forest Tuning - Determine "mtree", eval=FALSE, include=FALSE}

# Deside `mtry`
rf_mtry = 31

control <- trainControl(method="repeatedcv", number = 10, repeats = 3, 
                        search="grid"
                        # search="random"
                        )

tunegrid <- expand.grid(.mtry = c(rf_mtry))
mtree_list = c(500, 1000, 1500, 2000, 2500, 5000)
modellist <- list()
for (ntree in mtree_list) {
	set.seed(seed_num)
    fit = train(y~., data = train_split.combined, 
                   method = 'rf', 
                   metric = rf_metric, 
                   tuneGrid = tunegrid,
                   ntree = ntree,
                   trControl = control)
	key = toString(ntree)
	modellist[[key]] <- fit
}
# compare results
rf_results <- resamples(modellist)
summary(rf_results)
dotplot(rf_results)

```

`r if(model_exist) {"\\begin{comment}"}`

```{r Run Random Forest, echo=FALSE}

# Deside `mtry` and `mtree`
# NOTE - Change the value based on "obove" tunning results
rf_mtry = 20
rf_mtree = 500

print(qq("Training Dataset Summary"))
summary(train_split.combined)

# Run Random Forest against the "combined" Training data
# na.action = na.exclude
bag.data.combined = randomForest(y~.,
                         # data = data, subset = train_indices,
                         data = train_split.combined,
                         importance = TRUE,
                         ntry = rf_mtry,
                         ntree = rf_mtree,
                         na.action = na.roughfix)

# Save the model to file
if (params$working_folder != '') {
    setwd(params$working_folder)
}
if (params$model_file != '') {
    saveRDS(bag.data.combined, params$model_file)
}
```

`r if(model_exist) {"\\end{comment}"}`

```{r Random Forest Model Info, echo=FALSE}

print(qq("Random Forest Model Info"))
print(bag.data.combined)

```

### Prediction Performance

```{r Run Prediction, echo=FALSE}

# Overall performance
yhat.bag.train.combined = predict(bag.data.combined, newdata = train_split.combined)
yhat.bag.validation.combined = predict(bag.data.combined, newdata = validation_split.combined)
mse.train.combined = mean((yhat.bag.train.combined - train_split.combined$y)^2)
mse.validation.combined = mean((yhat.bag.validation.combined - validation_split.combined$y)^2)
print(qq("MSE - Training data (combined): @{mse.train.combined}"))

toplot_train.combined = data.frame(yhat.bag.train.combined, train_split.combined$y)
names(toplot_train.combined) = c("Predicted","Truth")
toplot_validation.combined = data.frame(yhat.bag.validation.combined, validation_split.combined$y)
names(toplot_validation.combined) = c("Predicted","Truth")

# Output "combined" result - only need if having more than "1" dataset
if (length(dataset_meta) > 1 ) {
    print(qq("MSE - Validation data (combined): @{mse.validation.combined}"))
    
    # 
    # print(qq("Training - Validation Confusion Matrix and Statistics"))
    # print(confusionMatrix(yhat.bag.validation.combined, validation_split.combined$y))
}

# For single dataset
for (dataset_codename in dataset_meta) {
    #
    validation_split.name = qq("validation_split.@{dataset_codename}")
    validation_split = get(validation_split.name)
    
    yhat.bag.validation = predict(bag.data.combined, newdata = validation_split)
    mse.validation = mean((yhat.bag.validation - validation_split$y)^2)
    
    # Build dataset to help plot
    toplot_validation.name = qq("toplot_validation.@{dataset_codename}")
    assign(toplot_validation.name,
           data.frame(yhat.bag.validation, validation_split$y)
           )
    toplot_validation = get(toplot_validation.name)
    names(toplot_validation) = c("Predicted","Truth")
    assign(toplot_validation.name,toplot_validation)
    
    #
    print(qq("MSE - Validation data (@{dataset_codename}): @{mse.validation}"))
}

# For "Testing" dataset
for (dataset_codename in testing_dataset_meta) {
    #
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    # Fix Levels
    list[train, test] <- fixLevels(train_split.combined, data_test)
    assign(data_test.name, test)
    data_test = get(data_test.name)
    
    yhat.bag.test = predict(bag.data.combined, newdata = data_test)
    mse.test = mean((yhat.bag.test - data_test$y)^2)
    
    # Build dataset to help plot
    toplot_test.name = qq("toplot_test.@{dataset_codename}")
    assign(toplot_test.name,
           data.frame(yhat.bag.test, data_test$y)
           )
    toplot_test = get(toplot_test.name)
    names(toplot_test) = c("Predicted","Truth")
    assign(toplot_test.name,toplot_test)
    
    #
    print(qq("MSE - Test data (@{dataset_codename}): @{mse.test}"))
}
```

#### Training vs. Validation

```{r Plot - Training & Validation - 1, fig.width=10, fig.height=10, echo=FALSE}

gen_plot <- function(dataset, dataset.name, xlab, ylab) {
    #
    # Calc "Coefficients"
    ret <- lm(Predicted ~ Truth, dataset)
    r_2 = format(summary(ret)$r.squared, digits = 3)
    mse = format(mean(ret$residuals^2), digits = 3)
    performance.text = qq("r^2 = @{r_2} | mse = @{mse}")
    
    # 
    # dataset.name = deparse(substitute(dataset))
    print(qq("@{dataset.name} - @{performance.text}"))
    
    plot = ggplot(data = dataset, 
                aes(x = Truth, y = Predicted)) +
            stat_smooth(method ="lm", se = FALSE) +
            geom_point(alpha = 0.3) +
            geom_text(x = 0.5, y = 1, label = performance.text, parse = FALSE) +
            xlab(xlab) +
            ylab(ylab) +
            xlim(c(0, 1)) +
            ylim(c(0, 1)) + 
            theme_bw(
                base_size = 8     # Base font-size
            ) +
            coord_equal()
    
    return(plot)
}

#
plots = list()

p.train.combined = gen_plot(toplot_train.combined, 'Training.combined',
                            'Training True', 'Training Predicted')
plots = list.append(plots, p.train.combined)
    
# Plot for "combined validation data" - only need if having more than "1" dataset
if (length(dataset_meta) > 1 ) {
    p.validation.combined = gen_plot(toplot_validation.combined, 'Validation.combined',
                                     'Validation True (combined)', 'Validation Predicted (combined)')
    plots = list.append(plots, p.validation.combined)
}

# For single dataset
for (dataset_codename in dataset_meta) {
    #
    toplot_validation.name = qq("toplot_validation.@{dataset_codename}")
    toplot_validation = get(toplot_validation.name)
    p.validation = gen_plot(toplot_validation, qq("Validation.@{dataset_codename}"),
                            qq('Validation True (@{dataset_codename})'), qq('Validation Predicted (@{dataset_codename})'))
    plots = list.append(plots, p.validation)
}
# Plot Grid
multiplot(plotlist = plots, cols = 2)
```

#### Training vs. Testing

```{r Plot - Training & Testing - 1, fig.width=10, fig.height=20, echo=FALSE}

if (length(testing_dataset_meta) > 0) {
    #
    plots = list()
    
    p.train.combined = gen_plot(toplot_train.combined, 'Training.combined',
                                'Training True', 'Training Predicted')
    plots = list.append(plots, p.train.combined)
    
    # For "testing" dataset
    for (dataset_codename in testing_dataset_meta) {
        #
        toplot_test.name = qq("toplot_test.@{dataset_codename}")
        toplot_test = get(toplot_test.name)
        p.test = gen_plot(toplot_test, qq("Test.@{dataset_codename}"),
                          qq('Test True (@{dataset_codename})'), qq('Test Predicted (@{dataset_codename})'))
        plots = list.append(plots, p.test)
    }
    # Plot Grid
    multiplot(plotlist = plots, cols = 2)
}
```

### Feature Importance

```{r Output Feature Importance - 1, echo=FALSE}

#get the feature importance 
feature_importance = as.data.frame(importance(bag.data.combined))
feature_importance = feature_importance[ order(-feature_importance[,1]), ]

print(feature_importance)

number_features_required = 15
if (length(row.names(bag.data.combined$importance)) < number_features_required) {
    number_features_required = length(row.names(bag.data.combined$importance))
}
varImpPlot(bag.data.combined, n.var=number_features_required, main="Feature Importance")
```

