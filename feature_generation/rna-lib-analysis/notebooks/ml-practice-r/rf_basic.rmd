---
output:
    html_document:
        toc: yes
params:
    title: 'Neil1_Computational_Test_with_Degenerate'
    working_folder: '/Users/cowfox/Desktop/_rna_lib_analysis/__analysis/2018_12_06-ML_Random_Forest'
    # Use "comma" to sparate
    dataset_codenames: 'neil1_computational'
    # dataset_codenames: 'neil1_computational, ajuba_bc_computational, ttyh2_bc_computational, ttyh2_ecs_bc_computational, ttyh2_computational'
    # Training-test dataset split
    training_validation_split: 0.8

    # External "Testing" dataset
    external_test_title: 'Degerate_Computational'
    # external_test_dataset_codename: 'neil1_degenerate_computational, ttyh2_bc_degenerate_computational'
    external_test_dataset_codename: 'neil1_degenerate_computational'
# title: "Prediction of Adar Editing Levels - `r params$data_name` (`r params$data_type`)"
title: "Prediction of Adar Editing Levels - `r params$title`"
---

```{r "knitr config", cache = FALSE, include=FALSE}
require("knitr")
# 
script_folder_path = getwd()
if (params$working_folder != '') {
    # Set it "globally"
    opts_knit$set(root.dir = params$working_folder)
}

# Global Chunk Options
# Ref - https://yihui.name/knitr/options/
opts_chunk$set(
    echo = TRUE
)
```

```{r Info, include=FALSE}
# ## Purpose of This Script
# It aims to run "Random Forest" on the RNA Liv data. 
# 
# ## Tutorial of Rmarkdown
# - https://bookdown.org/yihui/rmarkdown/
```

```{r Libraries, include=FALSE}
#
library(knitr)

# Random Forest
library(mlbench)
library(caret)
library(randomForest)
library(gbm)

library(ggplot2)

# Tools
library(rlist)
library(gtools)
library(gsubfn)
library(GetoptLong) # String interpolation
```

```{r Setup, include=FALSE}
# Clear - NO NEED. It needs "passing params"
# rm(list=ls())

# Global options
options(warn=-1)

# Local scripts
source(normalizePath(file.path(script_folder_path, './helper.R')))
```


```{r Prep, include=FALSE}
# Set the "Seed" for following "Random Forest" running
set.seed(166)
```

# Datasets Pre-processing

The following datasets are used as "training" & "validation": 

```{r  List datasets - training & validation, echo=FALSE }

# Build the list of "dataset codenames"
dataset_meta = c(
    'neil1_computational'
)
if (params$dataset_codenames != '') {
    dataset_meta = as.list(strsplit(params$dataset_codenames, ",")[[1]])
    dataset_meta = lapply(dataset_meta, trimws)     # Remove "space"
}

# 
for (dataset_codename in dataset_meta) {
    #
    print(qq("- @{dataset_codename}"))
}

```

The following datasets are used for "testing": 

```{r  List datasets - testing, echo=FALSE }

# Build the list of "testing dataset codenames"
testing_dataset_meta = c(
    'neil1_degenerate_computational'
)
if (params$external_test_dataset_codename != '') {
    testing_dataset_meta = as.list(strsplit(params$external_test_dataset_codename, ",")[[1]])
    testing_dataset_meta = lapply(testing_dataset_meta, trimws)     # Remove "space"
}

# 
for (dataset_codename in testing_dataset_meta) {
    #
    print(qq("- @{dataset_codename}"))
}

```

## Original Datasets Summary

```{r Load "feature" data files, echo=FALSE, warning=FALSE}

# Temp fix
# - Use `setwd()` tp set the "working folder" since `opts_knit$set(root.dir = params$working_folder)` not working.
# - Ignore "warning" in this chunk
if (params$working_folder != '') {
    setwd(params$working_folder)
}

# Loop into "codenames" and build the "dataset variables"
for (dataset_codename in dataset_meta) {
    # file path
    file_path = qq("./_output/feature_matrices/@{dataset_codename}.features.csv")
    assign(qq("feature_file_relative_path.@{dataset_codename}"), file_path)
    
    # original data
    data_orig = read.csv(file=file_path, header=TRUE, sep=",")
    assign(qq("data_orig.@{dataset_codename}"), data_orig)
}

# Summary Orig Datasets
for (dataset_codename in dataset_meta) {
    #
    print(qq("Original Dataset Summary - @{dataset_codename}"))
    
    dataset.name = qq("data_orig.@{dataset_codename}")
    print(summary(get(dataset.name)))
    # print(summary(eval(as.symbol(dataset_codename))))
}

# Testing dataset
for (dataset_codename in testing_dataset_meta) {
    # file path
    file_path = qq("./_output/feature_matrices/@{dataset_codename}.features.csv")
    assign(qq("feature_file_relative_path.@{dataset_codename}"), file_path)
    
    # original data
    data_orig = read.csv(file=file_path, header=TRUE, sep=",")
    assign(qq("data_orig.@{dataset_codename}"), data_orig)
}
```

# Prediction with "Random Forest"

Approaches: 

- Mutation-specific and Structure-Specific Features
- Structure-Specific Features


## Prediction from Both Mutation-Specific and Structure-Specific Features

### Trainig-Validation Split

The "Training split" is ``r params$training_validation_split``. 

```{r Traing-Test Split - 1, include=FALSE}

# Determine "Training" and "Test" set
# - Training set - a combined set of all "training" set from each data set
# - Test set - a combined set of all "training" set from each data set

data.combined = data.frame()
train_split.combined = data.frame()
validation_split.combined = data.frame()
for (dataset_codename in dataset_meta) {
    #
    dataset.name = qq("data_orig.@{dataset_codename}")
    dataset = get(dataset.name)
    
    train_indices.name = qq("train_indices.@{dataset_codename}")
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    
    assign(train_indices.name, 
           sample(nrow(dataset), as.double(params$training_validation_split) * nrow(dataset), replace=FALSE)
           )
    train_indices = get(train_indices.name)
    
    # Split
    assign(train_split.name, 
           dataset[ train_indices, ]
           )
    assign(validation_split.name, 
           dataset[ -train_indices, ]
           )
    
    data.combined = smartbind(data.combined, get(dataset.name))
    train_split.combined = smartbind(train_split.combined, get(train_split.name))
    validation_split.combined = smartbind(validation_split.combined, get(validation_split.name))
    
    # print(summary(data.combined))
}

# For "Test" dataset
for (dataset_codename in testing_dataset_meta) {
    #
    dataset.name = qq("data_orig.@{dataset_codename}")
    dataset = get(dataset.name)
    
    dataset_test.name = qq("data.test.@{dataset_codename}")
    
    assign(dataset_test.name, dataset)
}

# Keep the "levels" of overall "original" datasets
levels.data.combined = sapply(data.combined, levels)
```

### Processed Data Summary

```{r Process Dataset - 1, include=FALSE}

# "Column Names" to be dropped in the ourput dataset. 
column_names_to_drop = c("rna_id")
# "Column Names" to be processed the "data value issue"
column_names_to_correct = c()

# Combined
data.combined = prepData(data.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
train_split.combined = prepData(train_split.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
validation_split.combined = prepData(validation_split.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)

# For each dataset
for (dataset_codename in dataset_meta) {
    # 
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    assign(train_split.name,
           prepData(train_split, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
           )
    assign(validation_split.name,
           prepData(validation_split, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
           )
    
    # Convert "editing level" with "logarithm" calc
    # data$y = log2(data$y / (1 - data$y))
}

# For each "testing" dataset
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    assign(data_test.name,
           prepData(data_test, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
           )
}

# Fix the "Levals" of data frames to ensure "ALL" datasets are in the "SAME" levels. 
# 
# NOTE:
#   - The "fix" includes three steps: 
#       - Ensure the "processed combined" dataset has the "SAME" levels of "original combined" dataset. 
#       - If having the "testing" datasets, check them first, since they may bring some extra "levels". 
#       - Check if need to update the "subset" datasets of the "combined" dataset. 
# 

# For the "processed combined" dataset
for (col in names(data.combined)) {
    #
    if( length(levels(data.combined[ ,col ])) <= length(levels.data.combined[col]) ) {
        levels(data.combined[ ,col ]) = levels.data.combined[[col]]
    }
}

# For "testing" datasets
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    list[data, test] <- fixLevels(data.combined, data_test)
    data.combined <<- data
    assign(data_test.name, test)
}

# For subset datasets
list[data.combined, train_split.combined] <- fixLevels(data.combined, train_split.combined)
list[data.combined, validation_split.combined] <- fixLevels(data.combined, validation_split.combined)
for (dataset_codename in dataset_meta) {
    #
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    #
    list[data, train] <- fixLevels(data.combined, train_split)
    data.combined <<- data
    assign(train_split.name, train)
    
    # 
    list[data, validation] <- fixLevels(data.combined, validation_split)
    data.combined <<- data
    assign(validation_split.name, validation)
}
```

### Run Random Forest

```{r Rondom Forest Tuning - 1, eval=FALSE}

control <- trainControl(method="repeatedcv", number = 10, repeats = 3)
tunegrid <- expand.grid(
    .mtry=c(1:sqrt(ncol(train_split.combined) - 1)), 
    .ntree=c(500, 1000, 1500, 2000, 2500, 5000)
)
set.seed(seed)
rf_gridsearch <- train(y~., data = train_split.combined, 
                        method = 'rf', 
                        metric = metric, tuneGrid = tunegrid, trControl = control)
summary(rf_gridsearch)
plot(rf_gridsearch)

```


```{r Run Random Forest - 1, echo=FALSE}

print(qq("Training Dataset Summary"))
summary(train_split.combined)

# Run Random Forest against the "combined" Training data
# na.action = na.exclude
bag.data.combined <- randomForest(y~., 
                         # data = data, subset = train_indices, 
                         data = train_split.combined,
                         importance = TRUE, 
                         ntree = 10000, 
                         na.action = na.roughfix)

# 
print(qq("Random Forest Results"))
print(bag.data.combined)
```

### Prediction Performance

```{r Run Prediction - 1, echo=FALSE}

# Overall performance
yhat.bag.train.combined = predict(bag.data.combined, newdata = train_split.combined)
yhat.bag.validation.combined = predict(bag.data.combined, newdata = validation_split.combined)
mse.train.combined = mean((yhat.bag.train.combined - train_split.combined$y)^2)
mse.validation.combined = mean((yhat.bag.validation.combined - validation_split.combined$y)^2)
print(qq("MSE - Training data (combined): @{mse.train.combined}"))

toplot_train.combined = data.frame(yhat.bag.train.combined, train_split.combined$y)
names(toplot_train.combined) = c("TrainingSplitPredicted","TrainingSplitTruth")
toplot_validation.combined = data.frame(yhat.bag.validation.combined, validation_split.combined$y)
names(toplot_validation.combined) = c("TrainingSplitPredicted","TrainingSplitTruth")

# Output "combined" result - only need if having more than "1" dataset
if (length(dataset_meta) > 1 ) {
    print(qq("MSE - Validation data (combined): @{mse.validation.combined}"))
}

# For single dataset
for (dataset_codename in dataset_meta) {
    #
    validation_split.name = qq("validation_split.@{dataset_codename}")
    validation_split = get(validation_split.name)
    
    yhat.bag.validation = predict(bag.data.combined, newdata = validation_split)
    mse.validation = mean((yhat.bag.validation - validation_split$y)^2)
    
    # Build dataset to help plot
    toplot_validation.name = qq("toplot_validation.@{dataset_codename}")
    assign(toplot_validation.name,
           data.frame(yhat.bag.validation, validation_split$y)
           )
    toplot_validation = get(toplot_validation.name)
    names(toplot_validation) = c("TrainingSplitPredicted","TrainingSplitTruth")
    assign(toplot_validation.name,toplot_validation)
    
    #
    print(qq("MSE - Validation data (@{dataset_codename}): @{mse.validation}"))
}

# For "Testing" dataset
for (dataset_codename in testing_dataset_meta) {
    #
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)

    yhat.bag.test = predict(bag.data.combined, newdata = data_test)
    mse.test = mean((yhat.bag.test - data_test$y)^2)
    
    # Build dataset to help plot
    toplot_test.name = qq("toplot_test.@{dataset_codename}")
    assign(toplot_test.name,
           data.frame(yhat.bag.test, data_test$y)
           )
    toplot_test = get(toplot_test.name)
    names(toplot_test) = c("TrainingSplitPredicted","TrainingSplitTruth")
    assign(toplot_test.name,toplot_test)
    
    #
    print(qq("MSE - Test data (@{dataset_codename}): @{mse.test}"))
}
```

#### Training vs. Validation

```{r Plot - Training & Validation - 1, echo=FALSE}

gen_plot <- function(dataset, xlab, ylab) {
    #
    # Calc "Coefficients"
    ret <- lm(TrainingSplitPredicted ~ TrainingSplitTruth, dataset)
    r_2 = format(summary(ret)$r.squared, digits = 3)
    r_2.text = qq("r^2 = @{r_2}")
    
    plot = ggplot(data = dataset, 
                aes(x = TrainingSplitTruth, y = TrainingSplitPredicted)) +
            stat_smooth(method ="lm", se = FALSE) +
            geom_point(alpha = 0.3) +
            geom_text(x = 0.5, y = 1, label = r_2.text, parse = FALSE) +
            xlab(xlab) +
            ylab(ylab) +
            xlim(c(0, 1)) +
            ylim(c(0, 1)) + 
            theme_bw(
                base_size = 8     # Base font-size
            ) +
            coord_equal()
    
    return(plot)
}

#
plots = list()

p.train.combined = gen_plot(toplot_train.combined, 'Training True', 'Training Predicted')
plots = list.append(plots, p.train.combined)
    
# Plot for "combined validation data" - only need if having more than "1" dataset
if (length(dataset_meta) > 1 ) {
    p.validation.combined = gen_plot(toplot_validation.combined, 'Validation True (combined)', 'Validation Predicted (combined)')
    plots = list.append(plots, p.validation.combined)
}

# For single dataset
for (dataset_codename in dataset_meta) {
    #
    toplot_validation.name = qq("toplot_validation.@{dataset_codename}")
    toplot_validation = get(toplot_validation.name)
    p.validation = gen_plot(toplot_validation, qq('Validation True (@{dataset_codename})'), qq('Validation Predicted (@{dataset_codename})'))
    plots = list.append(plots, p.validation)
}
# Plot Grid
multiplot(plotlist = plots, cols = 2)
```

#### Training vs. Testing

```{r Plot - Training & Testing - 1, echo=FALSE}

if (length(testing_dataset_meta) > 0) {
    #
    plots = list()
    
    p.train.combined = gen_plot(toplot_train.combined, 'Training True', 'Training Predicted')
    plots = list.append(plots, p.train.combined)
    
    # For "testing" dataset
    for (dataset_codename in testing_dataset_meta) {
        #
        toplot_test.name = qq("toplot_test.@{dataset_codename}")
        toplot_test = get(toplot_test.name)
        p.test = gen_plot(toplot_test, qq('Test True (@{dataset_codename})'), qq('Test Predicted (@{dataset_codename})'))
        plots = list.append(plots, p.test)
    }
    # Plot Grid
    multiplot(plotlist = plots, cols = 2)
}
```

### Feature Importance

```{r Output Feature Importance - 1, echo=FALSE}

#get the feature importance 
feature_importance = as.data.frame(importance(bag.data.combined))
feature_importance = feature_importance[ order(-feature_importance[,1]), ]

print(feature_importance)

varImpPlot(bag.data.combined, n.var=10, main="Feature Importance")
```


## Prediction from Structure-Specific Features Only

All "mutation" related features will be removed from the dataset. 

```{r Remove "Mutation" features - 2, include=FALSE}

column_names.mutation = c("num_mutations", "mut_exist", "mut_type", "mut_pos", "mut_site_dist", "mut_ref_nt", "mut_nt", 
                          "mut_struct", "mut_prev_struct", "mut_next_struct", "mut_same_as_site")
```

### Trainig-Validation Split

The "Training split" is ``r params$training_validation_split``. 

```{r Traing-Test Split - 2, include=FALSE}

# Determine "Training" and "Test" set
# - Training set - a combined set of all "training" set from each data set
# - Test set - a combined set of all "training" set from each data set

data.combined = data.frame()
train_split.combined = data.frame()
validation_split.combined = data.frame()
for (dataset_codename in dataset_meta) {
    #
    dataset.name = qq("data_orig.@{dataset_codename}")
    dataset = get(dataset.name)
    
    train_indices.name = qq("train_indices.@{dataset_codename}")
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    
    assign(train_indices.name, 
           sample(nrow(dataset), as.double(params$training_validation_split) * nrow(dataset), replace=FALSE)
           )
    train_indices = get(train_indices.name)
    
    # Split
    assign(train_split.name, 
           dataset[ train_indices, ]
           )
    assign(validation_split.name, 
           dataset[ -train_indices, ]
           )
    
    data.combined = smartbind(data.combined, get(dataset.name))
    train_split.combined = smartbind(train_split.combined, get(train_split.name))
    validation_split.combined = smartbind(validation_split.combined, get(validation_split.name))
    
    # print(summary(data.combined))
}

# For "Test" dataset
for (dataset_codename in testing_dataset_meta) {
    #
    dataset.name = qq("data_orig.@{dataset_codename}")
    dataset = get(dataset.name)
    
    dataset_test.name = qq("data.test.@{dataset_codename}")
    
    assign(dataset_test.name, dataset)
}
```

### Processed Data Summary

```{r Process Dataset - 2, include=FALSE}

# "Column Names" to be dropped in the ourput dataset. 
column_names_to_drop = c("rna_id", column_names.mutation)
# "Column Names" to be processed the "data value issue"
column_names_to_correct = c()

# Combined
data.combined = prepData(data.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
train_split.combined = prepData(train_split.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
validation_split.combined = prepData(validation_split.combined, column_to_predict='editing_value',
                    columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)

# For each dataset
for (dataset_codename in dataset_meta) {
    # 
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    assign(train_split.name,
           prepData(train_split, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
           )
    assign(validation_split.name,
           prepData(validation_split, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
           )
    
    # Convert "editing level" with "logarithm" calc
    # data$y = log2(data$y / (1 - data$y))
}

# For each "testing" dataset
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    assign(data_test.name,
           prepData(data_test, column_to_predict='editing_value',
                columns_to_drop = column_names_to_drop, columns_to_correct = column_names_to_correct)
           )
}

# Fix the "Levals"
# For each "testing" dataset
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    list[data, test] <- fixLevels(data.combined, data_test)
    data.combined <<- data
    assign(data_test.name, test)
}

list[data.combined, train_split.combined] <- fixLevels(data.combined, train_split.combined)
list[data.combined, validation_split.combined] <- fixLevels(data.combined, validation_split.combined)
for (dataset_codename in dataset_meta) {
    #
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    #
    list[data, train] <- fixLevels(data.combined, train_split)
    data.combined <<- data
    assign(train_split.name, train)
    
    # 
    list[data, validation] <- fixLevels(data.combined, validation_split)
    data.combined <<- data
    assign(validation_split.name, validation)
}

###
# For each "testing" dataset
for (dataset_codename in testing_dataset_meta) {
    # 
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)
    
    list[data, test] <- fixLevels(data.combined, data_test)
    data.combined <<- data
    assign(data_test.name, test)
}

# 
list[data.combined, train_split.combined] <- fixLevels(data.combined, train_split.combined)
list[data.combined, validation_split.combined] <- fixLevels(data.combined, validation_split.combined)
for (dataset_codename in dataset_meta) {
    #
    train_split.name = qq("train_split.@{dataset_codename}")
    validation_split.name = qq("validation_split.@{dataset_codename}")
    train_split = get(train_split.name)
    validation_split = get(validation_split.name)
    
    #
    list[data, train] <- fixLevels(data.combined, train_split)
    data.combined <<- data
    assign(train_split.name, train)
    
    # 
    list[data, validation] <- fixLevels(data.combined, validation_split)
    data.combined <<- data
    assign(validation_split.name, validation)
}
```

### Run Random Forest

```{r Run Random Forest - 2, echo=FALSE}

print(qq("Training Dataset Summary"))
summary(train_split.combined)

# Run Random Forest against the "combined" Training data
# na.action = na.exclude
bag.data.combined <- randomForest(y~., 
                         # data = data, subset = train_indices, 
                         data = train_split.combined,
                         importance = TRUE, 
                         ntree = 10000, 
                         na.action = na.roughfix)

# 
print(qq("Random Forest Results"))
print(bag.data.combined)
```

### Prediction Performance

```{r Run Prediction - 2, echo=FALSE}

# Overall performance
yhat.bag.train.combined = predict(bag.data.combined, newdata = train_split.combined)
yhat.bag.validation.combined = predict(bag.data.combined, newdata = validation_split.combined)
mse.train.combined = mean((yhat.bag.train.combined - train_split.combined$y)^2)
mse.validation.combined = mean((yhat.bag.validation.combined - validation_split.combined$y)^2)
print(qq("MSE - Training data (combined): @{mse.train.combined}"))

toplot_train.combined = data.frame(yhat.bag.train.combined, train_split.combined$y)
names(toplot_train.combined) = c("TrainingSplitPredicted","TrainingSplitTruth")
toplot_validation.combined = data.frame(yhat.bag.validation.combined, validation_split.combined$y)
names(toplot_validation.combined) = c("TrainingSplitPredicted","TrainingSplitTruth")

# Output "combined" result - only need if having more than "1" dataset
if (length(dataset_meta) > 1 ) {
    print(qq("MSE - Validation data (combined): @{mse.validation.combined}"))
}

# For single dataset
for (dataset_codename in dataset_meta) {
    #
    validation_split.name = qq("validation_split.@{dataset_codename}")
    validation_split = get(validation_split.name)
    
    yhat.bag.validation = predict(bag.data.combined, newdata = validation_split)
    mse.validation = mean((yhat.bag.validation - validation_split$y)^2)
    
    # Build dataset to help plot
    toplot_validation.name = qq("toplot_validation.@{dataset_codename}")
    assign(toplot_validation.name,
           data.frame(yhat.bag.validation, validation_split$y)
           )
    toplot_validation = get(toplot_validation.name)
    names(toplot_validation) = c("TrainingSplitPredicted","TrainingSplitTruth")
    assign(toplot_validation.name,toplot_validation)
    
    #
    print(qq("MSE - Validation data (@{dataset_codename}): @{mse.validation}"))
}

# For "Testing" dataset
for (dataset_codename in testing_dataset_meta) {
    #
    data_test.name = qq("data.test.@{dataset_codename}")
    data_test = get(data_test.name)

    yhat.bag.test = predict(bag.data.combined, newdata = data_test)
    mse.test = mean((yhat.bag.test - data_test$y)^2)
    
    # Build dataset to help plot
    toplot_test.name = qq("toplot_test.@{dataset_codename}")
    assign(toplot_test.name,
           data.frame(yhat.bag.test, data_test$y)
           )
    toplot_test = get(toplot_test.name)
    names(toplot_test) = c("TrainingSplitPredicted","TrainingSplitTruth")
    assign(toplot_test.name,toplot_test)
    
    #
    print(qq("MSE - Test data (@{dataset_codename}): @{mse.test}"))
}
```

#### Training vs. Validation

```{r Plot - Training & Validation - 2, echo=FALSE}

gen_plot <- function(dataset, xlab, ylab) {
    #
    # Calc "Coefficients"
    ret <- lm(TrainingSplitPredicted ~ TrainingSplitTruth, dataset)
    r_2 = format(summary(ret)$r.squared, digits = 3)
    r_2.text = qq("r^2 = @{r_2}")
    
    plot = ggplot(data = dataset, 
                aes(x = TrainingSplitTruth, y = TrainingSplitPredicted)) +
            stat_smooth(method ="lm", se = FALSE) +
            geom_point(alpha = 0.3) +
            geom_text(x = 0.5, y = 1, label = r_2.text, parse = FALSE) +
            xlab(xlab) +
            ylab(ylab) +
            xlim(c(0, 1)) +
            ylim(c(0, 1)) + 
            theme_bw(
                base_size = 8     # Base font-size
            ) +
            coord_equal()
    
    return(plot)
}

#
plots = list()

p.train.combined = gen_plot(toplot_train.combined, 'Training True', 'Training Predicted')
plots = list.append(plots, p.train.combined)
    
# Plot for "combined validation data" - only need if having more than "1" dataset
if (length(dataset_meta) > 1 ) {
    p.validation.combined = gen_plot(toplot_validation.combined, 'Validation True (combined)', 'Validation Predicted (combined)')
    plots = list.append(plots, p.validation.combined)
}

# For single dataset
for (dataset_codename in dataset_meta) {
    #
    toplot_validation.name = qq("toplot_validation.@{dataset_codename}")
    toplot_validation = get(toplot_validation.name)
    p.validation = gen_plot(toplot_validation, qq('Validation True (@{dataset_codename})'), qq('Validation Predicted (@{dataset_codename})'))
    plots = list.append(plots, p.validation)
}
# Plot Grid
multiplot(plotlist = plots, cols = 2)
```

#### Training vs. Testing

```{r Plot - Training & Testing - 2, echo=FALSE}

if (length(testing_dataset_meta) > 0) {
    #
    plots = list()
    
    p.train.combined = gen_plot(toplot_train.combined, 'Training True', 'Training Predicted')
    plots = list.append(plots, p.train.combined)
    
    # For "testing" dataset
    for (dataset_codename in testing_dataset_meta) {
        #
        toplot_test.name = qq("toplot_test.@{dataset_codename}")
        toplot_test = get(toplot_test.name)
        p.test = gen_plot(toplot_test, qq('Test True (@{dataset_codename})'), qq('Test Predicted (@{dataset_codename})'))
        plots = list.append(plots, p.test)
    }
    # Plot Grid
    multiplot(plotlist = plots, cols = 2)
}
```

### Feature Importance

```{r Output Feature Importance - 2, echo=FALSE}

#get the feature importance 
feature_importance = as.data.frame(importance(bag.data.combined))
feature_importance = feature_importance[ order(-feature_importance[,1]), ]

print(feature_importance)

varImpPlot(bag.data.combined, n.var=10, main="Feature Importance")
```